---
title: Worker List
description: Learn how to use the /get_endpoint_workers/ and /get_autogroup_workers/ endpoints to retrieve a list of GPU instances under an Endpoint and Worker Group. Understand the inputs, outputs, and examples for using the endpoints.
"canonical": "/documentation/serverless/worker-list"
---

<script type="application/ld+json" dangerouslySetInnerHTML={{
  __html: JSON.stringify({
    "@context": "https://schema.org",
    "@type": "TechArticle",
    "headline": "Vast.ai Serverless Worker List API",
    "description": "API reference for retrieving GPU instances using /get_endpoint_workers/ and /get_autogroup_workers/ endpoints, including worker metrics (cur_load, cur_perf, disk_usage, reliability, status) and cURL examples.",
    "author": {
      "@type": "Organization",
      "name": "Vast.ai"
    },
    "articleSection": "Serverless Documentation",
    "keywords": ["worker list", "API", "GPU instances", "metrics", "monitoring", "serverless", "vast.ai"]
  })
}} />

The `/get_endpoint_workers/` and `/get_autogroup_workers/` endpoints return a list of GPU instances under an Endpoint and \{\{Worker\_Group}}, respectively.&#x20;

# https\://run.vast.ai/get\_endpoint\_workers/

## Inputs

- `id` (int): The id value of the Endpoint.
- `api_key` (string): The Vast API key associated with the account that controls the Endpoint.

The `api_key` could alternatively be provided in the request header as a bearer token.

```json JSON icon="js"
{
    "id": 123,
    "api_key": "$API_KEY"
}
```

## Outputs

For each GPU instance in the Endpoint, the following will be returned:

- `cur_load`(float): Current load (as defined by the PyWorker) the GPU instance is receiving per second.
- `new_load` (float): Amount of load the GPU instance received recently.
- `cur_load_rolling_avg`(float): Rolling average of `cur_load`.
- `cur_perf`(float): The most recent or current operational performance level of the instance (as defined by the PyWorker). For example, a text generation model has the units of tokens generated per second.
- `disk_usage`(float): Storage used by instance (in Gb).
- `dlperf`(float): Measured DLPerf of the instance. DLPerf is explained [here.](/documentation/reference/faq/index)
- `id`(int): Instance ID.
- `loaded_at`(float): Unix epoch time the instance finished loading.
- `measured_perf`(float): Benchmarked performances (tokens/s). Set to DLPerf if instance is not benchmarked.
- `perf`(float): `measured_perf` \* `reliability`.
- `reliability`(float): Uptime of the instance, ranges 0-1.
- `reqs_working`(int): Number of active requests currently being processed by the instance.
- `status`(string): Current status of the worker.

```json JSON icon="js"
{
    "cur_load": 150,
    "new_load": 50,
    "cur_load_rolling_avg": 50,
    "cur_perf": 80,
    "disk_usage": 30,
    "dlperf": 105.87206734930771,
    "id": 123456,
    "loaded_at": 1724275993.997,
    "measured_perf": 105.87206734930771,
    "perf": 100.5784639818423245,
    "reliability": 0.95,
    "reqs_working": 2,
    "status": "running"
}
```

## Example

### Python

```python
from vastai import Serverless
import asyncio

async def main():
    async with Serverless() as client:
        endpoint = await client.get_endpoint("test")
        workers = await endpoint.get_workers()
        for worker in workers:
            print(worker)

asyncio.run(main())
```

### curl

Run the following Bash command in a terminal to receive Endpoint workers.

```bash Bash
curl https://run.vast.ai/get_endpoint_workers/ \
-X POST \
-d '{"id" : 123, "api_key" : "API_KEY_HERE"}' \
-H 'Content-Type: application/json'
```

***

# https\://run.vast.ai/get\_autogroup\_workers/

## Inputs

- `id` (int): The id value of the Worker Group.
- `api_key` (string): The Vast API key associated with the account that controls the Endpoint.

The `api_key` could alternatively be provided in the request header as a bearer token.

```json JSON icon="js"
{
    "id": 1001,
    "api_key": "$API_KEY"
}
```

## Outputs

For each GPU instance in the Worker Group, the following will be returned:

- `cur_load`(float): Current load (as defined by the PyWorker) the GPU instance is receiving per second.
- `new_load` (float): Amount of load the GPU instance received recently.
- `cur_load_rolling_avg`(float): Rolling average of `cur_load`.
- `cur_perf`(float): The most recent or current operational performance level of the instance (as defined by the PyWorker). For example, a text generation model has the units of tokens generated per second.
- `disk_usage`(float): Storage used by instance (in Gb).
- `dlperf`(float): Measured DLPerf of the instance. DLPerf is explained [here.](/documentation/reference/faq/index)
- `id`(int): Instance ID.
- `loaded_at`(float): Unix epoch time the instance finished loading.
- `measured_perf`(float): Benchmarked performances (tokens/s). Set to DLPerf if instance is not benchmarked.
- `perf`(float): `measured_perf` \* `reliability`.
- `reliability`(float): Uptime of the instance, ranges 0-1.
- `reqs_working`(int): Number of active requests currently being processed by the instance.
- `status`(string): Current status of the worker.

```json JSON icon="js"
{
    "cur_load": 150,
    "new_load": 50,
    "cur_load_rolling_avg": 50,
    "cur_perf": 80,
    "disk_usage": 30,
    "dlperf": 105.87206734930771,
    "id": 123456,
    "loaded_at": 1724275993.997,
    "measured_perf": 105.87206734930771,
    "perf": 100.5784639818423245,
    "reliability": 0.95,
    "reqs_working": 2,
    "status": "running"
}
```

## Example

Run the following Bash command in a terminal to receive Worker Group workers.

```bash Bash
curl https://run.vast.ai/get_autogroup_workers/ \
-X POST \
-d '{"id" : 1001, "api_key" : "API_KEY_HERE"}' \
-H 'Content-Type: application/json'
```