---
title: Quickstart
description: Deploy your first vLLM endpoint
---

## Prerequisites

Before you begin, make sure you have:

<CardGroup cols={3}>
  <Card title="Vast.ai Account" icon="user">
    Sign up at [cloud.vast.ai](https://cloud.vast.ai) and add credits to your account
  </Card>
  <Card title="API Key" icon="key">
    Generate an API key from your [account settings](https://docs.vast.ai/keys)
  </Card>
  <Card title="HuggingFace Token" icon="robot">
    Create a [HuggingFace account](https://huggingface.co) and generate a [read-access token](https://huggingface.co/settings/tokens) for gated models
  </Card>
</CardGroup>

## Configuration

### Install the Vast SDK

Install the SDK that you'll use to interact with your serverless endpoints:

```bash
pip install vastai_sdk
```

<Note>
  The SDK provides an async Python interface for making requests to your endpoints. You'll use this after setting up your infrastructure.
</Note>

### HuggingFace Token Setup

Many popular models like Llama and Mistral require authentication to download. Configure your HuggingFace token once at the account level:

1. Navigate to your [Account Settings](https://cloud.vast.ai/account/)
2. Expand the **"Environment Variables"** section
3. Add a new variable:
   - **Key**: `HF_TOKEN`
   - **Value**: Your HuggingFace read-access token
4. Click the **"+"** button, then **"Save Edits"**

<Note>
  This token will be securely available to all your serverless workers. You only need to set it once for your account.
</Note>

<Warning>
  Without a valid HF_TOKEN, workers will fail to download gated models and remain in "Loading" state indefinitely.
</Warning>

## Deploy Your First Endpoint

<Steps>

  <Step title="Create an Endpoint">
    Navigate to the [Serverless Dashboard](https://cloud.vast.ai/serverless/) and click **"Create Endpoint"**.

    Use these recommended settings for your first deployment:

    | Setting | Value | Description |
    |---------|-------|-------------|
    | **Endpoint Name** | `vLLM-Qwen3-8B` | Choose a descriptive name for your endpoint |
    | **Cold Multiplier** | 3 | Scales capacity based on predicted load |
    | **Cold Workers** | 5 | Pre-loaded instances for instant scaling |
    | **Max Workers** | 16 | Maximum GPU instances |
    | **Minimum Load** | 3000 | Baseline tokens/second capacity |
    | **Target Utilization** | 0.9 | Resource usage target (90%) |

    ![](/images/serverless_quickstart_create_endpoint.png)

    Click **"Create"** to proceed.
  </Step>

  <Step title="Create a Workergroup">
    From the Serverless page, click **"+ Workergroup"** under your endpoint.

    Select the **vLLM (Serverless)** template, which comes pre-configured with:
    - **Model**: Qwen/Qwen3-8B (8 billion parameter LLM)
    - **Framework**: vLLM for high-performance inference
    - **API**: OpenAI-compatible endpoints

    The template will automatically select appropriate GPUs with enough VRAM for the model.

    ![](/images/serverless_quickstart_create_workergroup.png)

    Click **"Create"** to proceed with the default settings.
  </Step>

  <Step title="Wait for Workers to Initialize">
    Your serverless infrastructure is now being provisioned. **This process takes time** as workers need to:
    1. Start up the GPU instances
    2. Download the model (8GB for Qwen3-8B)
    3. Load the model into GPU memory
    4. Complete health checks

    <Warning>
      **Expect 3-5 minutes wait time** for workers to become ready, especially on first deployment. Larger models may take longer.
    </Warning>

    Monitor the worker status in the dashboard:
    - **Inactive**: Worker is initializing or downloading the model
    - **Loading**: Worker is loading the model into GPU memory
    - **Ready**: Worker is available to handle requests (this is what you're waiting for)

    You can view detailed statistics by clicking **"View detailed stats"** on the Workergroup.

    ![](/images/serverless_quickstart_loading_workers.png)

    <Note>
      Do not attempt to make API calls until at least one worker shows "Ready" status. API calls made before workers are ready will fail with "No workers available" errors.
    </Note>
  </Step>
</Steps>

## Make Your First API Call

### Basic Usage

With the SDK installed, here's how to make your first API call:

```python
import asyncio
from vastai_sdk import Serverless
import os

# Set your API key (or use VAST_API_KEY environment variable)
API_KEY = ""  # Replace with your actual API key
MAX_TOKENS = 100

async def main():
    # Initialize the client
    client = Serverless(API_KEY)  # or just Serverless() if VAST_API_KEY is set

    # Get your endpoint
    endpoint = await client.get_endpoint(name="vLLM-Qwen3-8B")

    # Prepare your request payload
    payload = {
        "input": {
            "model": "Qwen/Qwen3-8B",
            "prompt": "Explain quantum computing in simple terms",
            "max_tokens": 100,
            "temperature": 0.7
        }
    }

    # Make the request
    result = await endpoint.request("/v1/completions", payload, cost=MAX_TOKENS)

    # The SDK returns a wrapper object with metadata
    # Access the OpenAI-compatible response via result["response"]
    print(result["response"]["choices"][0]["text"])

    # Clean up
    await client.close()

if __name__ == "__main__":
    asyncio.run(main())
```

<Note>
  The SDK handles all the routing, worker assignment, and authentication automatically. You just need to specify your endpoint name and make requests.
</Note>

## Troubleshooting

<AccordionGroup>
  <Accordion title="Workers stuck in 'Loading' state">
    - Check if the GPU has enough VRAM for your model
    - Verify your model name is correct
    - Check worker logs in the dashboard by clicking on the worker
    - Ensure your HF_TOKEN is properly configured for gated models
  </Accordion>
  <Accordion title="'No workers available' error">
    - Wait 30-60 seconds for workers to initialize
    - Check endpoint status in the [Serverless Dashboard](https://cloud.vast.ai/serverless/)
    - Verify you have active workers showing "Ready" status
  </Accordion>
  <Accordion title="SSL certificate warnings">
    - This is expected with self-signed certificates
    - For development: Use `verify=False` in requests
    - For production: Install Vast's root certificate
  </Accordion>
  <Accordion title="Slow response times">
    - First request to a cold worker takes longer
    - Increase `cold_workers` for better availability
    - Check if workers are in the same region as your users
  </Accordion>
</AccordionGroup>

---

<Note>
  **Need help?** Join our [Discord community](https://discord.gg/hSuEbSQ4X8) or check the [detailed documentation](/serverless/architecture) for advanced configurations.
</Note>