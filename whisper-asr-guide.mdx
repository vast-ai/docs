---
title: Whisper ASR Guide
slug: dHQJ-title-whisper-asr-guide-slug-6ou-w-createdat-2025-04-07t094158045z-updatedat-2025-04-17t032758381z-whisper-is-a-general-purpose-speech-recognition-model-trained-on-a-large-dat
createdAt: Thu Apr 17 2025 03:27:50 GMT+0000 (Coordinated Universal Time)
updatedAt: Thu Apr 17 2025 03:30:55 GMT+0000 (Coordinated Universal Time)
---

<script type="application/ld+json" dangerouslySetInnerHTML={{
  __html: JSON.stringify({
    "@context": "https://schema.org",
    "@type": "HowTo",
    "name": "How to Use Whisper ASR on Vast.ai",
    "description": "A step-by-step guide to using Whisper automatic speech recognition web service on Vast.ai for audio transcription and language detection.",
    "step": [
      {
        "@type": "HowToStep",
        "name": "Select Whisper Template",
        "text": "Go to the templates tab and search for 'Whisper' or click the provided link to the Whisper ASR Webservice template. After selecting the template by pressing the triangle button, the next step is to choose a GPU."
      },
      {
        "@type": "HowToStep",
        "name": "Select a GPU Offering and Rent",
        "text": "Select a GPU from the search results that meets your requirements. The template provides access to both Jupyter and SSH, plus the Instance Portal web interface via the Open button. Click Rent to launch your instance."
      },
      {
        "@type": "HowToStep",
        "name": "Install TLS Certificate and Access Instance",
        "text": "HTTP and token-based auth are enabled by default. To avoid certificate errors, install the TLS certificate by following the instructions for secure HTTPS connections to your instance via its IP. Use the open button to open the instance. Default username: vastai, password: OPEN_BUTTON_TOKEN environment variable value (or run 'echo $OPEN_BUTTON_TOKEN' in terminal)."
      },
      {
        "@type": "HowToStep",
        "name": "Access SwaggerUI",
        "text": "After accessing the Instance Portal, click the triangle button to expand services, wait for the page to load (can take 5-10 minutes), then click into the link aligning with SwaggerUI. You should see the Swagger documentation interface."
      },
      {
        "@type": "HowToStep",
        "name": "Use Whisper Endpoints",
        "text": "Two POST endpoints are exposed: /detect-language (automatically detect spoken language in audio file) and /asr (transcription and translation of audio files). Both endpoints are documented using OpenAPI standard and can be tested in web browser. Select an endpoint, click 'try it out', upload an audio clip, and press execute. Check response body for results. Note: If getting internal 500 error, the file selected might be too large."
      }
    ]
  })
}} />

**Whisper** is a general-purpose speech recognition model trained on a large dataset of diverse audio. Go through the [Readme](https://cloud.vast.ai/template/readme/0c0c7d65cd4ebb2b340fbce39879703b) first before using.&#x20;

**Connecting to the Instance**


1. Go to the templates tab and search for “*Whisper*” or click the provided link to the template [here](https://cloud.vast.ai/?ref_id=62897\&creator_id=62897\&name=Whisper%20ASR%20Webservice) .&#x20;
2. After you select the template by pressing the triangle button the next step is to choose a gpu.

![](/images/use-cases-audio-to-text.png)

3\. **Select a GPU Offering&#x20;**

![](/images/use-cases-audio-to-text-2.png)

The template you selected will give your instance access to both Jupyter and SSH. Additionally the Open button will connect you to the instance portal web interface.&#x20;

4\. HTTP and token-based auth are both enabled by default. To avoid certificate errors in your browser, please follow the instructions for installing the TLS certificate [here](/documentation/instances/jupyter#1SmCz) to allow secure HTTPS connections to your instance via its IP.&#x20;

![](/images/use-cases-audio-to-text-3.png)

5\. Use the open button to open up the instance, if you are not using the open button the default username will be: vastai , and the password will be the value of the environment variable:*&#x20;OPEN\_BUTTON\_TOKEN*. You can also find the token value by accessing the terminal and executing this command: *echo $OPEN\_BUTTON\_TOKEN*

![](/images/use-cases-audio-to-text-4.png)

6\. After accessing the SwaggerUi by clicking the triangle button first then waiting for the page to load, then clicking into the link aligning with SwaggerUI you should see the page below. (note: usually loads fast but can take 5-10 minutes)&#x20;


![](/images/use-cases-audio-to-text-5.png)

**Usage**

Two POST endpoints are exposed in this template:

**/detect-language**

Use this endpoint to automatically detect the spoken language in a given audio file.

![](/images/use-cases-audio-to-text-6.png)

**/asr**

Use this endpoint for both transcription and translation of audio files.

*Both of these endpoints are documented using the OpenAPI standard and can be tested in a web browser.&#x20;*

![](/images/use-cases-audio-to-text-7.png)

7\. *Select the detect language endpoint*

8\. *Then click try it out.&#x20;*

![](/images/use-cases-audio-to-text-8.png)

9.*&#x20;From here upload an audio clip*&#x20;

10\. *Then press the execute button.&#x20;*

![](/images/use-cases-audio-to-text-9.png)

11.*&#x20;If you look in the response body (see below) you can see it was able to detect the language was English.*&#x20;

*Note: If you are getting an internal 500 error its most likely the file you selected to upload is to large.&#x20;*

![](/images/use-cases-audio-to-text-10.png)

*For more information and specifics on things such as but not limited to Configuration, Additional Functionality, Instance Logs, Cloudflared, Api request, ssh tunnels and port reference mapping, and Caddy you can visit the*[ Readme linked here to learn more. ](https://cloud.vast.ai/template/readme/0c0c7d65cd4ebb2b340fbce39879703b)


**Links**

- [GitHub Repository](https://github.com/ahmetoner/whisper-asr-webservice/)
- [Docker Image](https://hub.docker.com/r/onerahmet/openai-whisper-asr-webservice)

